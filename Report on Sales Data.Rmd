---
---
---

# Exploratory Data Analysis of Amazon Kindle Books

Sales is the most important aspect of any business; thus, all businesses need to pay attention to their sales data.

Sales data is any information that is human and machine-readable and is useful to sales teams and the organization. It aids in decision-making, greater client comprehension, and enhancing future performance within the organization.

Sales data can provide insightful information about a sales team's effectiveness, and aid in the creation of better strategies. It can also,

-   Help improve organizational decision-making,

-   Provide actionable information about a sales team's performance,

-   Assist sales representatives in avoiding pursuing customers who are a bad fit, and

-   Inform new opportunities that sales teams wouldn't otherwise detect, and assist in forecasting future sales.

Sales data should therefore not be neglected by any business or organization. In this report, I performed an exploratory data analysis on Amazon Kindle Books.

## Objectives

The objectives include to:

-   Determine the most popular authors based on the number of books they have released and their ratings.

-   Investigate the influence of Kindle Unlimited on book sales and popularity.

-   Assess the impact of the "Best Seller" and "Editors' Pick" tags on book sales.

-   Examine book publication trends over time and look for seasonal patterns.

-   Dive into various book categories to learn about their sales and popularity.

## Data Exploration

We'll begin our data exploration by installing and loading all necessary packages required to perform our analysis.

```{r}
#Install R packages
install.packages("tidyverse")
install.packages("ggplot2")
install.packages("readr")
install.packages("reshape2")
install.packages("pivottabler")
install.packages("gridExtra")
install.packages("corrplot")
install.packages("stargazer")

#Load R packages
library(tidyverse)
library(readr)
library(ggplot2)
library(gridExtra)
library(dplyr)
library(corrplot)
library(stargazer)
```

### Importing and inspecting our data

After loading our packages, the first thing we should do is to import our data set and inspect its structure.

```{r}
#Import sales data for exploration
salesData <- read.csv("kindle_data-v2.csv")
dim(salesData)

str(salesData)

head(salesData)
#To clearly look at the data in a tabular form
View(salesData)
```

From the above, we can see that our data has 16 variables and 133102 observations.

A critical look at each of the variables shows that some of the data types are not correct. So, let us normalize them before we proceed.

```{r}
#Normalize data types to fit the data
#convert isKindleunlimited to bool data type
salesData$isKindleUnlimited <- as.logical(salesData$isKindleUnlimited)
#Check if the data type has changed
class(salesData$isKindleUnlimited)

#convert isBestSeller, iseditorsPick, and isGood.. to bool data type
salesData$isBestSeller <- as.logical(salesData$isBestSeller)
salesData$isEditorsPick <- as.logical(salesData$isEditorsPick)
salesData$isGoodReadsChoice <- as.logical(salesData$isGoodReadsChoice)

#convert publishedDate to date data type
salesData$publishedDate <- as.Date(salesData$publishedDate, "%d/%m/%Y")

#Let's check the structure of our data again
str(salesData)
```

So far, all our variables has been normalized to their correct data type.

### Checking for missing values

Our next task is to check for missing values in our data set.

```{r}
#Let's look for missing data
missing <- data.frame("TotalNa" = sapply(salesData, function(x) length(which(is.na(x))) )) %>%
  filter(TotalNa > 0) %>%
  arrange(TotalNa)
missing
```

This tells us that there's only one missing value in the authors column. We can go ahead to know the exact location of this missing data using the code:

```{r}
#Where is the missing data located
sapply(salesData, function(y) which(is.na(y)))
```

This tells us that the missing value is found in the author's column in row 130436. Meanwhile, let's inspect each column to know the uniqueness of each observation.

``` r
#Let's check summary of each column with duplicates by summarizing each column
#I'll use a for look to iterate through each column in the table

# Firstly, let's create an empty list to store the summary tables
dupTables <- list()

# Let's use the for loop to iterate through each column in the salesData
for (col_name in names(salesData)) {
  # Create a summary table for the current column
  summaryTable <- salesData %>%
    group_by(!!sym(col_name)) %>%
    summarise(Count = n()) %>%
    filter(Count > 1) %>% 
    arrange(desc(Count))
  
  # Store the summary table in the list with the column name as the list name
  dupTables[[col_name]] <- summaryTable
}
```

We used the for loop above to find out if there are duplicates in each column, if there are, we count the number of duplicates for each entry.

```{r}
#Are there duplicates in asin column
dup_asin <- dupTables[["asin"]]
dup_asin
```

This shows that there are no duplicates in *asin* column. all entries are unique.

```{r}
#Are there duplicates in the title column
dup_title <- dupTables[["title"]]
View(dup_title)
```

This shows that there are 1108 duplicated titles.

```{r}
#Are there duplicates in the author column
dup_author <- dupTables[["author"]]
View(dup_author)
```

This indicates that about 18511 authors appeared more than once. We also found the author named "Not Available". This means that 424 authors are missing in our data set.

```{r}
#Let's check for other columns sequentially
dupTables[["soldBy"]]
dupTables[["imgUrl"]]
dupTables[["productURL"]]
dupTables[["stars"]]
dupTables[["reviews"]]
dupTables[["price"]]
dupTables[["isKindleUnlimited"]]
dupTables[["category_id"]]
dupTables[["isBestSeller"]]
dupTables[["isEditorsPick"]]
dupTables[["isGoodReadsChoice"]]
dupTables[["publishedDate"]]
dupTables[["category_name"]]
```

The above codes show that *soldBy* has 46 duplicates with "Not Available" appearing 9233 times. The remainder of the columns has the following duplicates:

| Column Name       | No. of Duplicates | No. of Missing Values |
|-------------------|-------------------|-----------------------|
| imgUrl            | 83                | 3                     |
| productURL        | 0                 | 0                     |
| stars             | 37                | 0                     |
| reviews           | 4369              | 0                     |
| price             | 2785              | 0                     |
| isKindleUnlimited | 2                 | 0                     |
| category_id       | 31                | 0                     |
| isBestSeller      | 2                 | 0                     |
| isEditorsPick     | 2                 | 0                     |
| isGoodReadsChoice | 2                 | 0                     |
| publishedDate     | 5574              | 49016                 |
| category_name     | 31                | 0                     |

The above shows that we have missing data to deal with in the *authors,* *soldBy*, and *publishedDate* columns. This is in addition to the NA earlier detected in the authors column.

How do we handle these missing data? The easiest way is to remove the associated rows. But before we proceed, we should have a justifiable reason to take such measure.

So, let us extract all the missing data in our data set.

```{r}
#Let's select only the missing data in our dataset
missing_data <- subset(salesData, author == "Not Available" | soldBy == "Not Available" | imgUrl == "No Image Found" | publishedDate == "0001-01-01")
str(missing_data)

```

This gives us a total of 55302 missing data. This is quite huge. If we decide to delete all rows with a missing data, we'll have 77800 observations left, which is not too bad.

On the other hand, if we want to input all 55302 missing data, we may introduce bias in our analysis which could lead to misleading results.

Looking at the variables, you should understand that *author* and *soldBy* variables are critical to our analysis. What I would do is to remove all observations that has missing values in *author* and *soldBy* columns.

For our *publishedDate* variable, we can input random dates between the already existing date interval.

### Dealing with missing data

Firstly, let us remove the rows that coincide with all the missing variables.

```{r}
#Rows with 3 of the missing variables: author, soldBy and publishedDate
missing_var <- subset(salesData, author == "Not Available" & 
                         soldBy == "Not Available" & 
                         publishedDate == "0001-01-01")
View(missing_var)
```

This gave us 31 observations that do not have author, soldBy and publishedDate. Thus, we remove these rows and have a total of 133071 left.

Let's now remove those 31 observations from our data set. We have a new data set called **salesData_rev**.

```{r}
salesData_rev <- subset(salesData, author !="Not Available" |
                          soldBy != "Not Available" |
                          publishedDate != "0001-01-01")
View(salesData_rev)
```

Secondly, let's remove all rows that have missing authors. But how many authors are missing in our data?

```{r}
author_NA <- salesData_rev %>% 
  filter(author == "Not Available") %>% 
  select(asin, author)
str(author_NA)
```

So, 393 authors are missing. Let's remove these rows from our data.

```{r}
#Let's remove the rows where author is not aavailable
salesData_rev <- subset(salesData_rev, author != "Not Available")

#Let's be sure that all NA authors have been removed
head(subset(salesData_rev, author == "Not Available" | author == "NA"))
```

Thirdly, we'll remove all rows that have missing soldBy data.

```{r}
#How many rows do not have soldBy data
soldBy_NA <- subset(salesData_rev, soldBy == "Not Available")
#Remove the missing data from the data set
salesData_rev <- subset(salesData_rev, soldBy != "Not Available")
#check to be sure we've removed all NA
head(subset(salesData_rev, soldBy == "Not Available" | soldBy == "NA"))
```

Now, we have a total of 123564 observations in our data set. Let's check the total number of missing date data left.

```{r}
#No. of missing date data
publishedDate_NA <- salesData_rev %>% 
  filter(publishedDate == "0001-01-01") %>% 
  select(asin, author, publishedDate)
str(publishedDate_NA)
```

That's a total of 45764 observations. We need to input these values using the interval between the beginning and ending date in our data.

Let's first get the beginning and ending dates.

```{r}
#Let's get the beginning and ending date using max and min f(x)
publishedDate_allowed <- salesData_rev %>% 
  filter(publishedDate != "0001-01-01") #remove the 0001-01-01 date
print(max(publishedDate_allowed$publishedDate)) #get maximum date = 2023-10-17
print(min(publishedDate_allowed$publishedDate)) #get minimum date = 1900-01-01
```

We got the range 1900-01-01 to 2023-10-17. But, I am wondering if 1900-01-01 is right or an outlier. So, I want to know how close other dates are to it.

```{r}
#Let's group the dates to know how close they are to each other
publishedDate_group <- publishedDate_allowed %>% 
  group_by(publishedDate) %>% #group by publishedDate
  summarise(count = n()) %>% #count how many rows has such date
  arrange(publishedDate) #sort by published date in ascending order
head(publishedDate_group)
```

The result shows that we have only one data with such date.

```{r}
#The title, author and genre of the book
theBook <- publishedDate_allowed %>% 
  filter(publishedDate == "1900-01-01") %>% 
  select(asin, title, author, category_name, publishedDate)
theBook
```

A Google search revealed that this book was first published in 1982-01-01, see [Goodreads](https://www.goodreads.com/book/show/389530.Chimpanzee_Politics). A check on a few books shows that their dates are correct. so, we'll change the 1900-01-01 to 1982-01-01.

```{r}
#Let's change the date 1900-01-01 to 1982-01-01
publishedDate_allowed["publishedDate"][publishedDate_allowed["publishedDate"] == 
                                         "1900-01-01"] <- 1982-01-01
salesData_rev["publishedDate"][salesData_rev["publishedDate"] == 
                                 "1900-01-01"] <- 1982-01-01
#Check to be sure the value has been replaced
head(subset(salesData_rev, publishedDate == "1900-01-01"))
```

Let's now generate random date between 1942-02-01 and 2023-10-01, and use them to replace the missing values.

```{r}
#Let's generate a random date between 2023-10-01 and 1942-02-01
#1. Check min date in date column after removing 1900-01-01
print(min(publishedDate_allowed$publishedDate))
#2. Replace 0001-01-01 with NA to account for missing values
salesData_rev["publishedDate"][salesData_rev["publishedDate"] == 
                                "0001-01-01"] <- NA
#3. Count the number of missing values to be sure is 45764
print(sum(is.na(salesData_rev$publishedDate)))
#4. Generate 45764 random dates
set.seed(45001) #setting a seed to ensure the dates are reproducible
rd_dates <- as.Date(runif(45764, as.Date(1942-02-01),
                          as.Date(2023-10-01))) #generate the random dates
View(rd_dates)
#5. Replace the missing dates with the random dates
salesData_rev$publishedDate[is.na(salesData_rev$publishedDate)] <- rd_dates
#Check to be sure that values have been replaced
head(subset(salesData_rev, publishedDate == "0001-01-01"))
```

We are done with our missing values.

### Checking for Outliers and Data Anomalies

To check for outliers and anomalies in our data we use plots. Basically, we'll use histograms and boxplots.

```{r}
#Checking for outliers and anomalies in our data
#1. Plot a histogram for stars column
ggplot(salesData_rev, aes(stars)) +
  geom_histogram(bins = 50) +
  labs(title = "Histogram of Kindle Book Stars",
       x = "Stars Per Book",
       y = "Frequency")
```

We have an outlier of the value 0. But this is not a problem because it represents books are not readers' favourites. Since it's part of the data, we'll leave it.

```{r}
#2. Checking for outliers in reviews
ggplot(salesData_rev, aes(reviews)) +
  geom_histogram(bins = 50) +
  labs(title = "Histogram of Kindle Books Reviews", 
       x = "No of Reviews", 
       y = "No of Books")
```

For reviews, there are no outliers.

```{r}
#3. Check for outliers in price
ggplot(salesData_rev, aes(x = price)) +
  geom_boxplot() +
  labs(title = "Detecting Outliers in Price Variable", 
       x = "Price of Books")
```

There are no major outliers in the prices of books. Though there are indications of outliers in the plot, but I believe that they are not out of ordinary. To inquire into the data, let's know how many books has prices above \$500.

```{r}
#Checking the No. of books with prices higher than $500
expensiveBooks <- salesData_rev %>% 
  filter(price > 500) %>% 
  summarise(count = n())
expensiveBooks
```

There are only 6 such books. Further, we try to investigate to findout their true prices online. These are the 6 books.

```{r}
#What are the books
expensiveBooks_1 <- salesData_rev %>% 
  filter(price > 500) %>% 
  select(title, author, price)
head(expensiveBooks_1)
```

Searching for them on [Amazon](https://www.amazon.com/s?k=Nobody+Loves+Me%3A+Bobby%E2%80%99s+true+story+of+neglect%2C+secrets+and+abuse&i=digital-text&crid=1Q3C991X2GLJP&sprefix=nobody+loves+me+bobby+s+true+story+of+neglect%2C+secrets+and+abuse%2Cdigital-text%2C352&ref=nb_sb_noss) shows that their prices are correct. Hence, nothing is out of ordinary.

## Correlations among Variables

So far we've completed our exploration. Before I create a dashboard to derive insights from the data, lets check for statistical relationships. To achieve this, we use correlation matrix and coefficients.

```{r}
#Creating a correlation matrix with discrete variables
#1. Select only numeric variables
salesData_Num <- salesData_rev %>% 
  select_if(., is.numeric) %>% 
  select(stars, reviews, price, category_id)
#2. Create the correlation matrix, rounding up to 4 decimal places
corMatrix <- round(cor(salesData_Num, use = "complete.obs"),4)
corMatrix[upper.tri(corMatrix)] <- NA
corMatrix
#3. Create a heatmap of the correlation matrix
corrplot(corMatrix, method = "color", type = "lower")
```

The correlation matrix shows that non of the variables are correlated. Thus, we can use these variables on multiple linear regression to predict price. This is because there'll be no trace of multicollinearity.

### Building a price prediction model

Let us finally build a model to predict the price of a book based on book genre and ratings. Book genre is represented by the category_id, and book ratings are determined by stars and reviews.

Therefore, our model is given by:

$Price = a + stars + reviews + category_id + u$

```{r}
#Creating a multiple regression model to predict price
priceModel <- lm(price ~ stars + reviews + category_id, salesData_Num)
#Display the results
summary(priceModel)
#Convert the results to table manually
reg_coefficients <- summary(priceModel)$coefficients
Reg_table <- as.data.frame(coeff)
print(Reg_table)

#Using the starhazer package to create a summary table
stargazer(priceModel, type = "text")
```

From the above result we have:

$Price = 35.952 - 3.759 stars - 0.0002 reviews - 0.218 category_id$

The result shows that all the coefficients in our model are statistically significant. Therefore, they can be used to predict the price of a book.

## Conclusion

Haven explored our data; we can now create a dashboard to draw insights from our data. I will use Looker Studio to create the dashboard.

Let's create a csv file to export our cleaned data so we can easily use it in any dashboard reporting tool.

```{r}
#Export the cleaned data "salesData_rev" to csv for use in Looker studio
write.csv(salesData_rev, 
          "C:\\Users\\Kmacims\\Desktop\\Movies\\Tutorials\\Projects\\Looker Studio\\KindleData_clean.csv")
```
